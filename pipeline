#!/usr/bin/env python
from __future__ import print_function

import argparse

from petmarrna.tasks import *

from doit.cmd_base import TaskLoader
from doit.doit_cmd import DoitMain

def run_tasks(tasks, args, config={'verbosity': 2}):

    if type(tasks) is not list:
        raise TypeError('tasks must be a list')

    class Loader(TaskLoader):
        @staticmethod
        def load_tasks(cmd, opt_values, pos_args):
            return tasks, config

    DoitMain(Loader()).run(args)

def get_database_prep_tasks(resources_df):
    '''
    Database prep, homology search

    First, we get remote flat files.
    '''
    tasks = []
    for key, row in resources_df[resources_df.access == 'remote_file'].iterrows():
        tasks.append(download_and_gunzip_task(row.url, row.filename))

    # Programmatically query uniprot
    for key, row in  resources_df[(resources_df.access == 'remote_query') & \
                                  (resources_df.q_type == 'uniprot')].iterrows():

        tasks.append(uniprot_query_task(row.terms, row.filename, label=row.filename))

    # Prep HMM profiles
    for key, row in resources_df[resources_df.meta_type == 'hmm_profiles'].iterrows():
        tasks.append(hmmpress_task(row.filename))

    # Truncate the long fasta names so blast doesn't choke
    for key, row in resources_df[resources_df.meta_type == 'fasta_database'].iterrows():
        tasks.append(truncate_fasta_header_task(row.filename))

    # Generate blast indices
    for key, row in resources_df[resources_df.meta_type.isin(['fasta_database', 'assembly'])].iterrows():
        tasks.append(blast_format_task(row.filename, row.filename + '.db', row.db_type))

    return tasks

def get_abundance_estimation_tasks(assembly_fn, sample_df, config):

    tasks = []

    '''
    The first task is to build the bowtie2 index for the assembly.
    We'll generate the basename from the assembly filename, then give it
    to the task creator.
    '''
    bt2_db_basename = strip_seq_extension(assembly_fn) + '.bt2idx'
    bt2_build_cfg = config['pipeline']['bowtie2_build']
    bt2_build_task = bowtie2_build_task(assembly_fn, bt2_db_basename, bt2_build_cfg)
    tasks.append(bt2_build_task)

    #split_tasks = sample_df[sample_df.paired == True].apply(split_pairs_task, axis=1, reduce=False)
    #tasks.extend(list(split_tasks))

    '''
    Now we generate the align and eXpress tasks for each sample
    '''
    express_files = []
    for key, row in sample_df.iterrows():
        target_fn = '{sample}.x.{idx}'.format(sample=row.filename, idx=bt2_db_basename)
        encoding = row.phred
        sample_fn = row.filename
        bowtie2_align_cfg = config['pipeline']['bowtie2_align']

        if row.paired == True:
            align_task = bowtie2_align_task(bt2_db_basename, target_fn, bowtie2_align_cfg,
                                            left_fn=sample_fn+'.1', right_fn=sample_fn+'.2',
                                            singleton_fn=sample_fn+'.0', encoding=encoding)
            tasks.append(align_task)
            # We have to sort results with samtools when we have paired input
            sort_task = samtools_sort_task(target_fn + '.bam')
            tasks.append(sort_task)
            # The final result has a rather verbose extension
            bam_fn = target_fn + '.bam.sorted.bam'
        else:
            align_task = bowtie2_align_task(bt2_db_basename, target_fn, bowtie2_align_cfg,
                                            singleton_fn=sample_fn, encoding=encoding)
            tasks.append(align_task)
            bam_fn = target_fn + '.bam'

        # Generate directory name for eXpress results
        express_dir = bam_fn.split('.bam')[0]
        express_task = eXpress_task(assembly_fn, bam_fn, express_dir)
        tasks.append(express_task)

        # Get the eXpress results file
        express_files.append(os.path.join(express_dir, 'results.xprs'))

    # Aggregate them
    agg_task = aggregate_express_task(express_files,
                                      '{pref}.eXpress.tpm.tsv'.format(pref=assembly_fn),
                                      '{pref}.eXpress.eff.tsv'.format(pref=assembly_fn),
                                      '{pref}.eXpress.tot.tsv'.format(pref=assembly_fn))
    tasks.append(agg_task)

    return tasks

def get_sample_prep_tasks(sample_df, config, data_dir):
    tasks = []

    # This list will track the files output by the first pass of digital normalization
    dg_files = []
    for key, row in sample_df.iterrows():
        # First, link all samples into the current directory
        tasks.append(link_file_task(os.path.join(data_dir, row.filename)))
        # Actions are different for paired and unpaired samples; paired files,
        # quite annoyingly, have to be split for Trimmomatic, then re-interleaved
        # for digital normalization
        if row.paired:
            # Split the paired file
            split_task = split_pairs_task(row.filename)
            tasks.append(split_task)

            # Get the trim config and create Trimmomatic tasks for pe and orphans
            trim_cfg = config['pipeline']['trimmomatic']
            left_fn = row.filename + '.1'
            right_fn = row.filename + '.2'
            orphan_fn = row.filename + '.0'
            trim_pe_task = trimmomatic_pe_task(left_fn, right_fn,
                                               left_fn + '.pe.trim', left_fn + '.se.trim',
                                               right_fn + '.pe.trim', right_fn + '.se.trim',
                                               row.phred, trim_cfg)
            trim_se_task = trimmomatic_se_task(orphan_fn, orphan_fn + '.trim',
                                               row.phred, trim_cfg)
            tasks.extend([trim_se_task, trim_pe_task])

            # Combine the original orphans and those created by Trimmomatic
            cat_se_task = cat_task([left_fn + '.se.trim', right_fn + '.se.trim', orphan_fn + '.trim'],
                                row.filename + '.qc.se.fq')
            tasks.append(cat_se_task)
            # Interleave the split-paired samples
            ilv_task = interleave_task(left_fn + '.pe.trim', right_fn + '.pe.trim',
                                              row.filename + '.qc.pe.fq')
            tasks.append(ilv_task)
            # Run digital normalization on paired and orphaned reads together
            dg_cfg = config['pipeline']['khmer']['normalize']['single']
            dg_task = diginorm_task([row.filename + '.qc.pe.fq', row.filename + '.qc.se.fq'],
                                    dg_cfg, row.filename + '.qc.fq.keep')
            tasks.append(dg_task)
            dg_files.extend([row.filename + '.qc.pe.fq.keep', row.filename + '.qc.se.fq.keep'])
        else:
            # Get trim config and build the Trimmomatic task for it
            trim_cfg = config['pipeline']['trimmomatic']
            trim_se_task = trimmomatic_se_task(row.filename, row.filename + '.qc.fq',
                                               row.phred, trim_cfg)
            tasks.append(trim_se_task)

            # Build the digital normalization task
            dg_cfg = config['pipeline']['khmer']['normalize']['single']
            dg_task = diginorm_task([row.filename + '.qc.fq'], dg_cfg,
                                    row.filename + '.qc.fq.keep')
            tasks.append(dg_task)
            dg_files.append(row.filename + '.qc.fq.keep')

    # Now run the pooled second pass of digital normalization
    dg_cfg = config['pipeline']['khmer']['normalize']['pooled']
    ct_fn = config['pipeline']['prefix'] + '.all-samples.pooled.ct'
    dg_label = config['pipeline']['prefix'] + '_all-samples_pooled'
    dg_task = diginorm_task(dg_files, dg_cfg, dg_label, ct_outfn=ct_fn)
    tasks.append(dg_task)

    # Finally, the filter-abund task
    fab_cfg = config['pipeline']['khmer']['filter-abund']
    fab_label = config['pipeline']['prefix'] + '_all-samples_pooled'
    fab_task = filter_abund_task([fn + '.keep' for fn in dg_files], ct_fn, fab_cfg, fab_label)
    tasks.append(fab_task)

    return tasks


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resources-metadata', default='resources.json')
    parser.add_argument('--config-metadata', default='config.json')
    parser.add_argument('--print-tasks', action='store_true', default=False)
    parser.add_argument('--local-file-dir', default='_data')
    parser.add_argument('--assembly-file', default='lamp10.fasta')
    args, doit_args = parser.parse_known_args()

    with open(args.resources_metadata, 'r') as fp:
        print('** Using data resources found in {c}'.format(c=args.resources_metadata), file=sys.stderr)
        resources = json.load(fp)
    with open(args.config_metadata, 'r') as fp:
        print('** Using config found in {c}'.format(c=args.config_metadata), file=sys.stderr)
        config = json.load(fp)

    desc = '''
####################################################################
#
# 2015 Petromyzon marinus (sea lamprey) de novo RNA-seq Pipeline
#
# * Authors:
#   {authors}
#
# * About:
#   {desc}
#
####################################################################
'''.format(authors=', '.join(config['meta']['authors']),
           desc=config['meta']['description'])
    print(desc, file=sys.stderr)

    assembly_fn = args.assembly_file

    work_dir = config['pipeline']['work_dir']
    local_dir = os.path.abspath(args.local_file_dir)

    resources_df = pd.DataFrame(resources).transpose()
    sample_df = resources_df[resources_df.meta_type == 'sample']

    old_dir = os.getcwd()
    try:
        if not os.path.exists(work_dir):
            os.makedirs(work_dir)
        os.chdir(work_dir)
        print('** Current Working Directory: {w}'.format(w=os.getcwd()), file=sys.stderr)

        tasks = []
        '''
        Sample preprocessing
        '''
        sample_prep_tasks = get_sample_prep_tasks(sample_df, config, local_dir)
        tasks.extend(sample_prep_tasks)
        tasks.append(group_task('preprocess', [t.name for t in sample_prep_tasks]))

        '''
        Reference database prep
        '''
        db_prep_tasks = get_database_prep_tasks(resources_df)
        tasks.extend(db_prep_tasks)
        tasks.append(group_task('databases', [t.name for t in db_prep_tasks]))

        '''
        BLAST
        '''
        #blast_iters = []
        #for fn in resources_df[resources_df.meta_type == 'assembly'].filename:
#
        #    blast_iters.extend([blast_task(row, config, fn) \
        #                for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
        #                                           (resources_df.meta_type != 'assembly')].iterrows()])
#
        #blast_iters.extend([blast_task(row, config, resources_df.ix['petMar2_cdna'].filename) \
        #                for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
        #                                           (resources_df.filename != 'petMar2.cdna.fa')].iterrows()])
#
        #blast_tasks = []
        #for tskiter in blast_iters:
        #    blast_tasks.extend([dict_to_task(tsk) for tsk in tskiter])
        #tasks.extend(blast_tasks)
        #tasks.append(group_task('blast', [t.name for t in blast_tasks]))

        '''
        TransDecoder and hmmscan
        '''
        tdc_tasks = []
        dbfn = resources_df.ix['pfamA'].filename
        tdc_tasks.append(transdecoder_orf_task(assembly_fn, config['pipeline']['transdecoder']))
        pep_fn = os.path.join(assembly_fn+'.transdecoder_dir', 'longest_orfs.pep')
        tdc_tasks.append(hmmscan_task(pep_fn, assembly_fn + '.pfam-A.out', dbfn,
                                      config['pipeline']['hmmscan']))
        tdc_tasks.append(transdecoder_predict_task(assembly_fn, assembly_fn + '.pfam-A.out',
                         config['pipeline']['transdecoder']))
        tasks.extend(tdc_tasks)
        tasks.append(group_task('transdecoder', [t.name for t in tdc_tasks]))

        '''
        BUSCO
        '''

        busco_cfg = config['pipeline']['busco']
        busco_tasks = []
        busco_vert_db_task = download_and_untar_task(busco_cfg['vert_url'],
                                                     busco_cfg['db_dir'],
                                                     label='vertebrata')
        busco_metz_db_task = download_and_untar_task(busco_cfg['metazoa_url'],
                                                     busco_cfg['db_dir'],
                                                     label='metazoa')
        busco_tasks.append(busco_vert_db_task)
        busco_tasks.append(busco_metz_db_task)

        for fn in [assembly_fn, resources_df.ix['petMar2_cdna'].filename]:
            for db in ['metazoa', 'vertebrata']:
                output_dir = '.'.join([fn, db, busco_cfg['output_suffix']])
                busco_tasks.append(busco_task(fn, output_dir,
                                   os.path.join(busco_cfg['db_dir'], db),
                                   'trans', busco_cfg))

        tasks.extend(busco_tasks)
        tasks.append(group_task('busco', [t.name for t in busco_tasks]))

        '''
        infernal and Rfam
        '''
        cmscan_cfg = config['pipeline']['cmscan']
        cmscan_tasks = []
        cmscan_tasks.append(download_and_untar_task(resources_df.ix['rfam'].url,
                                                    cmscan_cfg['db_dir'],
                                                    label='rfam'))



        '''
        Abundance estimation (bowtie2, eXpress)
        '''

        abundance_tasks = get_abundance_estimation_tasks(assembly_fn, sample_df, config)
        tasks.extend(abundance_tasks)
        tasks.append(group_task('abundance', [t.name for t in abundance_tasks]))


        #blast_targets = list(resources_df[(resources_df.meta_type == 'fasta_database') &
        #                                  (resources_df.meta_type != 'assembly')].filename)
        #ann_task = aggregate_annotations_task(assembly_fn, blast_targets,
        #                                      assembly_fn+'.transdecoder.gff3',
        #                                      assembly_fn+'.pfam-A.out', sample_df,
        #                                      assembly_fn+'.eXpress.tpm.tsv',
        #                                      assembly_fn+'.annotations.h5')
        #tasks.append(ann_task)


        if args.print_tasks:
            for task in tasks:
                print('-----\n', task)
                pprint.pprint(task.__dict__)

        if doit_args:
            run_tasks(tasks, doit_args)

    finally:
        os.chdir(old_dir)

main()
